{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tinygrad.helpers import Timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are the base data structure in tinygrad. They can be thought of as a multidimensional array of a specific data type. All high level operations in tinygrad operate on these tensors.\n",
    "\n",
    "The tensor class can be imported like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be created from an existing data structure like a python list or numpy ndarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = Tensor([1, 2, 3, 4, 5])\n",
    "na = np.array([1, 2, 3, 4, 5])\n",
    "t2 = Tensor(na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can also be created using one of the many factory methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = Tensor.full(shape=(2, 3), fill_value=5) # create a tensor of shape (2, 3) filled with 5\n",
    "zeros = Tensor.zeros(2, 3) # create a tensor of shape (2, 3) filled with 0\n",
    "ones = Tensor.ones(2, 3) # create a tensor of shape (2, 3) filled with 1\n",
    "\n",
    "full_like = Tensor.full_like(full, fill_value=2) # create a tensor of the same shape as `full` filled with 2\n",
    "zeros_like = Tensor.zeros_like(full) # create a tensor of the same shape as `full` filled with 0\n",
    "ones_like = Tensor.ones_like(full) # create a tensor of the same shape as `full` filled with 1\n",
    "\n",
    "eye = Tensor.eye(3) # create a 3x3 identity matrix\n",
    "arange = Tensor.arange(start=0, stop=10, step=1) # create a tensor of shape (10,) filled with values from 0 to 9\n",
    "\n",
    "rand = Tensor.rand(2, 3) # create a tensor of shape (2, 3) filled with random values from a uniform distribution\n",
    "randn = Tensor.randn(2, 3) # create a tensor of shape (2, 3) filled with random values from a standard normal distribution\n",
    "uniform = Tensor.uniform(2, 3, low=0, high=10) # create a tensor of shape (2, 3) filled with random values from a uniform distribution between 0 and 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are even more of these factory methods, you can find them in the Tensor Creation file.\n",
    "\n",
    "All the tensors creation methods can take a dtype argument to specify the data type of the tensor, find the supported dtype in dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import dtypes\n",
    "\n",
    "t3 = Tensor([1, 2, 3, 4, 5], dtype=dtypes.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors allow you to perform operations on them like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = Tensor([1, 2, 3, 4, 5])\n",
    "t5 = (t4 + 1) * 2\n",
    "t6 = (t5 * t4).relu().log_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these operations are lazy and are only executed when you realize the tensor using .realize() or .numpy()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-56. -48. -36. -20.   0.]\n"
     ]
    }
   ],
   "source": [
    "print(t6.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "Neural networks in tinygrad are really just represented by the operations performed on tensors. These operations are commonly grouped into the __call__ method of a class which allows modularization and reuse of these groups of operations. These classes do not need to inherit from any base class, in fact if they don't need any trainable parameters they don't even need to be a class!\n",
    "\n",
    "An example of this would be the nn.Linear class which represents a linear layer in a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, in_features, out_features, bias=True, initialization: str='kaiming_uniform'):\n",
    "    self.weight = getattr(Tensor, initialization)(out_features, in_features)\n",
    "    self.bias = Tensor.zeros(out_features) if bias else None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return x.linear(self.weight.transpose(), self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more neural network modules already implemented in nn, and you can also implement your own.\n",
    "\n",
    "We will be implementing a simple neural network that can classify handwritten digits from the MNIST dataset. Our classifier will be a simple 2 layer neural network with a Leaky ReLU activation function. It will use a hidden layer size of 128 and an output layer size of 10 (one for each digit) with no bias on either Linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNet:\n",
    "  def __init__(self):\n",
    "    self.l1 = Linear(784, 128, bias=False)\n",
    "    self.l2 = Linear(128, 10, bias=False)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.l1(x)\n",
    "    x = x.leakyrelu()\n",
    "    x = self.l2(x)\n",
    "    return x\n",
    "\n",
    "net = TinyNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the forward pass of our neural network is just the sequence of operations performed on the input tensor x. We can also see that functional operations like leakyrelu are not defined as classes and instead are just methods we can just call. Finally, we just initialize an instance of our neural network, and we are ready to start training it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now that we have our neural network defined we can start training it. Training neural networks in tinygrad is super simple. All we need to do is define our neural network, define our loss function, and then call .backward() on the loss function to compute the gradients. They can then be used to update the parameters of our neural network using one of the many Optimizers.\n",
    "\n",
    "For our loss function we will be using sparse categorical cross entropy loss. The implementation below is taken from tensor.py, it's copied below to highlight an important detail of tinygrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_categorical_crossentropy(self, Y, ignore_index=-1) -> Tensor:\n",
    "    loss_mask = Y != ignore_index\n",
    "    y_counter = Tensor.arange(self.shape[-1], dtype=dtypes.int32, requires_grad=False, device=self.device).unsqueeze(0).expand(Y.numel(), self.shape[-1])\n",
    "    y = ((y_counter == Y.flatten().reshape(-1, 1)).where(-1.0, 0) * loss_mask.reshape(-1, 1)).reshape(*Y.shape, self.shape[-1])\n",
    "    return self.log_softmax().mul(y).sum() / loss_mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in this implementation of cross entropy loss, there are certain operations that tinygrad does not support natively. Load/store ops are not supported in tinygrad natively because they add complexity when trying to port to different backends, 90% of the models out there don't use/need them, and they can be implemented like it's done above with an arange mask.\n",
    "\n",
    "For our optimizer we will be using the traditional stochastic gradient descent optimizer with a learning rate of 3e-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.nn.optim import SGD\n",
    "\n",
    "opt = SGD([net.l1.weight, net.l2.weight], lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we are passing in the parameters of our neural network to the optimizer. This is due to the fact that the optimizer needs to know which parameters to update. There is a simpler way to do this just by using get_parameters(net) from tinygrad.nn.state which will return a list of all the parameters in the neural network. The parameters are just listed out explicitly here for clarity.\n",
    "\n",
    "Now that we have our network, loss function, and optimizer defined all we are missing is the data to train on! There are a couple of dataset loaders in tinygrad located in /extra/datasets. We will be using the MNIST dataset loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extra.datasets import fetch_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to start training our neural network. We will be training for 1000 steps with a batch size of 64.\n",
    "\n",
    "We use with Tensor.train() set the internal flag Tensor.training to True during training. Upon exit, the flag is restored to its previous value by the context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Loss: 168.95492553710938 | Accuracy: 0.125\n",
      "Step 101 | Loss: 4.965189456939697 | Accuracy: 0.828125\n",
      "Step 201 | Loss: 7.4101104736328125 | Accuracy: 0.765625\n",
      "Step 301 | Loss: 2.3140299320220947 | Accuracy: 0.90625\n",
      "Step 401 | Loss: 1.0417046546936035 | Accuracy: 0.921875\n",
      "Step 501 | Loss: 2.562096118927002 | Accuracy: 0.90625\n",
      "Step 601 | Loss: 2.3611135482788086 | Accuracy: 0.875\n",
      "Step 701 | Loss: 2.6837334632873535 | Accuracy: 0.890625\n",
      "Step 801 | Loss: 3.420783758163452 | Accuracy: 0.84375\n",
      "Step 901 | Loss: 1.2426196336746216 | Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = fetch_mnist()\n",
    "\n",
    "with Tensor.train():\n",
    "  for step in range(1000):\n",
    "    # random sample a batch\n",
    "    samp = np.random.randint(0, X_train.shape[0], size=(64))\n",
    "    batch = Tensor(X_train[samp], requires_grad=False)\n",
    "    # get the corresponding labels\n",
    "    labels = Tensor(Y_train[samp])\n",
    "\n",
    "    # forward pass\n",
    "    out = net(batch)\n",
    "\n",
    "    # compute loss\n",
    "    loss = sparse_categorical_crossentropy(out, labels)\n",
    "\n",
    "    # zero gradients\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    opt.step()\n",
    "\n",
    "    # calculate accuracy\n",
    "    pred = out.argmax(axis=-1)\n",
    "    acc = (pred == labels).mean()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "      print(f\"Step {step+1} | Loss: {loss.numpy()} | Accuracy: {acc.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Now that we have trained our neural network we can evaluate it on the test set. We will be using the same batch size of 64 and will be evaluating for 1000 of those batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.89459375\n",
      "Time: 3743.36 ms\n"
     ]
    }
   ],
   "source": [
    "with Timing(\"Time: \"):\n",
    "  avg_acc = 0\n",
    "  for step in range(1000):\n",
    "    # random sample a batch\n",
    "    samp = np.random.randint(0, X_test.shape[0], size=(64))\n",
    "    batch = Tensor(X_test[samp], requires_grad=False)\n",
    "    # get the corresponding labels\n",
    "    labels = Y_test[samp]\n",
    "\n",
    "    # forward pass\n",
    "    out = net(batch)\n",
    "\n",
    "    # calculate accuracy\n",
    "    pred = out.argmax(axis=-1).numpy()\n",
    "    avg_acc += (pred == labels).mean()\n",
    "  print(f\"Test Accuracy: {avg_acc / 1000}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
